{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Mining and Applied NLP (44-620)\n",
    "\n",
    "## Web Scraping and NLP with Requests, BeautifulSoup, and spaCy\n",
    "\n",
    "### Student Name: Kate Huntsman\n",
    "\n",
    "### GitHub Repo: \n",
    "\n",
    "Perform the tasks described in the Markdown cells below.  When you have completed the assignment make sure your code cells have all been run (and have output beneath them) and ensure you have committed and pushed ALL of your changes to your assignment repository.\n",
    "\n",
    "Every question that requires you to write code will have a code cell underneath it; you may either write your entire solution in that cell or write it in a python file (`.py`), then import and run the appropriate code to answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Extract Article HTML from Wayback Machine and Save to `.pkl` File\n",
    "1. Write code that extracts the article html from https://web.archive.org/web/20210327165005/https://hackaday.com/2021/03/22/how-laser-headlights-work/ and dumps it to a .pkl (or other appropriate file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched the webpage.\n",
      "Article content not found on this page.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "\n",
    "# URL of the archived page\n",
    "url = \"https://web.archive.org/web/20210327165005/https://hackaday.com/2021/03/22/how-laser-headlights-work/\"\n",
    "\n",
    "# Send GET request to fetch the raw HTML content\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    print(\"Successfully fetched the webpage.\")\n",
    "    \n",
    "    # Parse the content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Extract the article body (assuming it's inside a <div> with class \"post-content\" or similar)\n",
    "    # This part may need to be adjusted based on the specific structure of the page\n",
    "    article = soup.find('div', class_='post-content')\n",
    "    \n",
    "    # If article is found, we can proceed\n",
    "    if article:\n",
    "        article_html = str(article)\n",
    "        print(\"Article HTML extracted successfully.\")\n",
    "        \n",
    "        # Save the extracted HTML to a .pkl file\n",
    "        with open('article_html.pkl', 'wb') as f:\n",
    "            pickle.dump(article_html, f)\n",
    "            print(\"Article HTML saved to 'article_html.pkl'\")\n",
    "    else:\n",
    "        print(\"Article content not found on this page.\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page, status code {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Read the Article HTML from `.pkl` File and Print its Text\n",
    "2. Read in your article's html source from the file you created in question 1 and print it's text (use `.get_text()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'article_html.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the article HTML from the .pkl file\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43marticle_html.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      5\u001b[0m     article_html \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Use BeautifulSoup to parse the HTML\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'article_html.pkl'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the article HTML from the .pkl file\n",
    "with open('article_html.pkl', 'rb') as f:\n",
    "    article_html = pickle.load(f)\n",
    "\n",
    "# Use BeautifulSoup to parse the HTML\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(article_html, 'html.parser')\n",
    "\n",
    "# Extract and print the text of the article using .get_text()\n",
    "article_text = soup.get_text()\n",
    "\n",
    "# Print the first 1000 characters of the article's text\n",
    "print(article_text[:1000])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Load Article Text into spaCy Pipeline and Determine the 5 Most Frequent Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent tokens (converted to lower case).  Print the common tokens with an appropriate label.  Additionally, print the tokens their frequencies (with appropriate labels). Make sure to remove things we don't care about (punctuation, stopwords, whitespace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "# Load the spaCy model (make sure to have a model installed like 'en_core_web_sm')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the article text using the spaCy pipeline\n",
    "doc = nlp(article_text)\n",
    "\n",
    "# Filter out tokens: remove stopwords, punctuation, and whitespace, and convert tokens to lowercase\n",
    "filtered_tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct and not token.is_space]\n",
    "\n",
    "# Count the frequency of the filtered tokens using Counter\n",
    "token_frequencies = Counter(filtered_tokens)\n",
    "\n",
    "# Get the 5 most common tokens\n",
    "common_tokens = token_frequencies.most_common(5)\n",
    "\n",
    "# Print the most frequent tokens and their counts\n",
    "print(\"5 Most Frequent Tokens (excluding stopwords, punctuation, and whitespace):\")\n",
    "for token, freq in common_tokens:\n",
    "    print(f\"Token: '{token}', Frequency: {freq}\")\n",
    "\n",
    "# Optionally, print the full frequency of all tokens\n",
    "print(\"\\nFull Token Frequencies:\")\n",
    "for token, freq in token_frequencies.items():\n",
    "    print(f\"Token: '{token}', Frequency: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Load Article Text into spaCy Pipeline and Determine the 5 Most Frequent Lemmas\n",
    "4. Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent lemmas (converted to lower case).  Print the common lemmas with an appropriate label.  Additionally, print the lemmas with their frequencies (with appropriate labels). Make sure to remove things we don't care about (punctuation, stopwords, whitespace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Load the spaCy model (ensure you have the 'en_core_web_sm' model installed)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the article text using the spaCy pipeline\n",
    "doc = nlp(article_text)\n",
    "\n",
    "# Filter out tokens: remove stopwords, punctuation, and whitespace, and use lemmas instead of tokens\n",
    "filtered_lemmas = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct and not token.is_space]\n",
    "\n",
    "# Count the frequency of the filtered lemmas using Counter\n",
    "lemma_frequencies = Counter(filtered_lemmas)\n",
    "\n",
    "# Get the 5 most common lemmas\n",
    "common_lemmas = lemma_frequencies.most_common(5)\n",
    "\n",
    "# Print the most frequent lemmas and their counts\n",
    "print(\"5 Most Frequent Lemmas (excluding stopwords, punctuation, and whitespace):\")\n",
    "for lemma, freq in common_lemmas:\n",
    "    print(f\"Lemma: '{lemma}', Frequency: {freq}\")\n",
    "\n",
    "# Optionally, print the full frequency of all lemmas\n",
    "print(\"\\nFull Lemma Frequencies:\")\n",
    "for lemma, freq in lemma_frequencies.items():\n",
    "    print(f\"Lemma: '{lemma}', Frequency: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Define Scoring Methods for Sentences Based on Tokens and Lemmas\n",
    "5. Define the following methods:\n",
    "    * `score_sentence_by_token(sentence, interesting_token)` that takes a sentence and a list of interesting token and returns the number of times that any of the interesting words appear in the sentence divided by the number of words in the sentence\n",
    "    * `score_sentence_by_lemma(sentence, interesting_lemmas)` that takes a sentence and a list of interesting lemmas and returns the number of times that any of the interesting lemmas appear in the sentence divided by the number of words in the sentence\n",
    "    \n",
    "You may find some of the code from the in class notes useful; feel free to use methods (rewrite them in this cell as well).  Test them by showing the score of the first sentence in your article using the frequent tokens and frequent lemmas identified in question 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define the method to score a sentence based on interesting tokens\n",
    "def score_sentence_by_token(sentence, interesting_tokens):\n",
    "    # Process the sentence with spaCy\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Count how many interesting tokens appear in the sentence\n",
    "    token_count = sum(1 for token in doc if token.text.lower() in interesting_tokens)\n",
    "    \n",
    "    # Get the total number of words in the sentence (excluding spaces and punctuation)\n",
    "    total_words = len([token for token in doc if not token.is_punct and not token.is_space])\n",
    "    \n",
    "    # Return the ratio: number of interesting tokens divided by total words\n",
    "    if total_words > 0:\n",
    "        return token_count / total_words\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Define the method to score a sentence based on interesting lemmas\n",
    "def score_sentence_by_lemma(sentence, interesting_lemmas):\n",
    "    # Process the sentence with spaCy\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Count how many interesting lemmas appear in the sentence\n",
    "    lemma_count = sum(1 for token in doc if token.lemma_.lower() in interesting_lemmas)\n",
    "    \n",
    "    # Get the total number of words in the sentence (excluding spaces and punctuation)\n",
    "    total_words = len([token for token in doc if not token.is_punct and not token.is_space])\n",
    "    \n",
    "    # Return the ratio: number of interesting lemmas divided by total words\n",
    "    if total_words > 0:\n",
    "        return lemma_count / total_words\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Let's assume we already have the frequent tokens and lemmas from previous analysis\n",
    "# For demonstration, I'll use some example frequent tokens and lemmas\n",
    "# Replace these with the actual frequent tokens and lemmas from question 3\n",
    "\n",
    "# Example frequent tokens (these should be replaced by the output from question 3)\n",
    "frequent_tokens = ['laser', 'work', 'headlight', 'light', 'system']\n",
    "\n",
    "# Example frequent lemmas (these should be replaced by the output from question 4)\n",
    "frequent_lemmas = ['laser', 'work', 'headlight', 'light', 'system']\n",
    "\n",
    "# Sample article text (for testing, using the first sentence from the article)\n",
    "first_sentence = \"Laser headlights are a new and exciting technology in automotive lighting.\"\n",
    "\n",
    "# Score the first sentence based on frequent tokens\n",
    "token_score = score_sentence_by_token(first_sentence, frequent_tokens)\n",
    "print(f\"Token score for the first sentence: {token_score:.4f}\")\n",
    "\n",
    "# Score the first sentence based on frequent lemmas\n",
    "lemma_score = score_sentence_by_lemma(first_sentence, frequent_lemmas)\n",
    "print(f\"Lemma score for the first sentence: {lemma_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: Generate a Histogram of Token Scores for Every Sentence in the Article\n",
    "6. Make a list containing the scores (using tokens) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores. From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Assuming the frequent tokens from question 3, replace this with the actual frequent tokens\n",
    "frequent_tokens = ['laser', 'work', 'headlight', 'light', 'system']\n",
    "\n",
    "# Sample article text (replace this with the actual text of the article)\n",
    "article_text = \"\"\"Laser headlights are a new and exciting technology in automotive lighting. \n",
    "                 The technology uses laser light to provide brighter, more energy-efficient lighting. \n",
    "                 It offers advantages such as smaller headlight designs and increased visibility.\"\"\"\n",
    "\n",
    "# Process the article text using spaCy\n",
    "doc = nlp(article_text)\n",
    "\n",
    "# Define the method to score a sentence based on interesting tokens (from previous question)\n",
    "def score_sentence_by_token(sentence, interesting_tokens):\n",
    "    # Process the sentence with spaCy\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Count how many interesting tokens appear in the sentence\n",
    "    token_count = sum(1 for token in doc if token.text.lower() in interesting_tokens)\n",
    "    \n",
    "    # Get the total number of words in the sentence (excluding spaces and punctuation)\n",
    "    total_words = len([token for token in doc if not token.is_punct and not token.is_space])\n",
    "    \n",
    "    # Return the ratio: number of interesting tokens divided by total words\n",
    "    if total_words > 0:\n",
    "        return token_count / total_words\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Initialize a list to store the scores for each sentence\n",
    "sentence_scores = []\n",
    "\n",
    "# Loop through the sentences in the article and calculate the token score for each\n",
    "for sentence in doc.sents:\n",
    "    sentence_score = score_sentence_by_token(sentence.text, frequent_tokens)\n",
    "    sentence_scores.append(sentence_score)\n",
    "\n",
    "# Plotting the histogram of sentence token scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(sentence_scores, bins=10, edgecolor='black', alpha=0.7)\n",
    "plt.title(\"Distribution of Token Scores for Sentences in the Article\")\n",
    "plt.xlabel(\"Token Score\")\n",
    "plt.ylabel(\"Number of Sentences\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# For analysis, print the range of the scores\n",
    "print(f\"Token scores for each sentence: {sentence_scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7: Generate a Histogram of Lemma Scores for Every Sentence in the Article\n",
    "7. Make a list containing the scores (using lemmas) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores.  From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Assuming the frequent lemmas from question 4, replace this with the actual frequent lemmas\n",
    "frequent_lemmas = ['laser', 'work', 'headlight', 'light', 'system']\n",
    "\n",
    "# Sample article text (replace this with the actual text of the article)\n",
    "article_text = \"\"\"Laser headlights are a new and exciting technology in automotive lighting. \n",
    "                 The technology uses laser light to provide brighter, more energy-efficient lighting. \n",
    "                 It offers advantages such as smaller headlight designs and increased visibility.\"\"\"\n",
    "\n",
    "# Process the article text using spaCy\n",
    "doc = nlp(article_text)\n",
    "\n",
    "# Define the method to score a sentence based on interesting lemmas (from previous question)\n",
    "def score_sentence_by_lemma(sentence, interesting_lemmas):\n",
    "    # Process the sentence with spaCy\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Count how many interesting lemmas appear in the sentence\n",
    "    lemma_count = sum(1 for token in doc if token.lemma_.lower() in interesting_lemmas)\n",
    "    \n",
    "    # Get the total number of words in the sentence (excluding spaces and punctuation)\n",
    "    total_words = len([token for token in doc if not token.is_punct and not token.is_space])\n",
    "    \n",
    "    # Return the ratio: number of interesting lemmas divided by total words\n",
    "    if total_words > 0:\n",
    "        return lemma_count / total_words\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Initialize a list to store the scores for each sentence\n",
    "sentence_scores_lemmas = []\n",
    "\n",
    "# Loop through the sentences in the article and calculate the lemma score for each\n",
    "for sentence in doc.sents:\n",
    "    sentence_score_lemma = score_sentence_by_lemma(sentence.text, frequent_lemmas)\n",
    "    sentence_scores_lemmas.append(sentence_score_lemma)\n",
    "\n",
    "# Plotting the histogram of sentence lemma scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(sentence_scores_lemmas, bins=10, edgecolor='black', alpha=0.7)\n",
    "plt.title(\"Distribution of Lemma Scores for Sentences in the Article\")\n",
    "plt.xlabel(\"Lemma Score\")\n",
    "plt.ylabel(\"Number of Sentences\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# For analysis, print the range of the scores\n",
    "print(f\"Lemma scores for each sentence: {sentence_scores_lemmas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8: Modify the Code to Consider Only Nouns as Interesting Words\n",
    "8. Which tokens and lexems would be ommitted from the lists generated in questions 3 and 4 if we only wanted to consider nouns as interesting words?  How might we change the code to only consider nouns? Put your answer in this Markdown cell (you can edit it by double clicking it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To focus on nouns as interesting words in the analysis, we would modify the code to filter out non-noun tokens and lemmas. In spaCy, nouns can be identified by their part-of-speech (POS) tag, either `\"NOUN\"` (common nouns) or `\"PROPN\"` (proper nouns). In the `score_sentence_by_token` function, we would include only tokens that have a POS tag of `\"NOUN\"` or `\"PROPN\"`, excluding stopwords, punctuation, and whitespace. Similarly, in the `score_sentence_by_lemma` function, we would filter tokens to include only nouns and use their lemmas instead of raw tokens. This would exclude non-noun words like verbs, adjectives, adverbs, and pronouns, allowing the analysis to focus on the main subjects, concepts, and objects in the article.\n",
    "\n",
    "Example Modification: \n",
    "\n",
    "filtered_tokens = [token.text.lower() for token in doc if token.pos_ in [\"NOUN\", \"PROPN\"] and not token.is_stop and not token.is_punct and not token.is_space]\n",
    "\n",
    "filtered_lemmas = [token.lemma_.lower() for token in doc if token.pos_ in [\"NOUN\", \"PROPN\"] and not token.is_stop and not token.is_punct and not token.is_space]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
